---
title: Basic LlamaCpp Agent
description: Basic example of using LlamaCpp with Agno
---

This example shows how to create a basic agent using LlamaCpp.

## Prerequisites

1. Install and set up LlamaCpp following the [setup guide](/concepts/models/llama_cpp#set-up-llamacpp)
2. Start the LlamaCpp server:

```bash
llama-server -hf ggml-org/gpt-oss-20b-GGUF --ctx-size 0 --jinja -ub 2048 -b 2048
```

## Example

<CodeGroup>

```python cookbook/models/llama_cpp/basic.py
from agno.agent import Agent, RunOutput  # noqa
from agno.models.llama_cpp import LlamaCpp

agent = Agent(model=LlamaCpp(id="ggml-org/gpt-oss-20b-GGUF"), markdown=True)

# Get the response in a variable
# run: RunOutput = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")
```

</CodeGroup>

## Key Features

- **Simple Setup**: Uses the default LlamaCpp server configuration
- **Markdown Support**: Enables markdown formatting in responses
- **Local Inference**: Runs entirely on your local machine
- **OpenAI-Compatible**: Uses the same interface as OpenAI models

## Running the Example

1. Ensure your LlamaCpp server is running
2. Run the example:

```bash
python cookbook/models/llama_cpp/basic.py
```

The agent will generate a creative 2-sentence horror story using your local model.

## Configuration

You can customize the model configuration:

<CodeGroup>

```python custom_basic.py
from agno.agent import Agent
from agno.models.llama_cpp import LlamaCpp

# Custom configuration
agent = Agent(
    model=LlamaCpp(
        id="your-custom-model",
        base_url="http://localhost:8080/v1",
        temperature=0.7,
        max_tokens=150
    ),
    markdown=True
)

agent.print_response("Tell me a joke")
```

</CodeGroup>
