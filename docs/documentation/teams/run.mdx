---
title: Running your Team
description: Learn how to run a team and get the response.
---

The `Team.run()` function runs the team and generates a response, either as a `TeamRunOutput` object or a stream of `TeamRunOutputEvent` objects.

<Note>
Many of our examples use `team.print_response()` which is a helper utility to print the response in the terminal. It uses `team.run()` under the hood.
</Note>

## Running your Team
Here's how to run your team. The response is captured in the `response` and `response_stream` variables.

```python
from agno.team import Team
from agno.models.openai import OpenAIChat

agent_1 = Agent(name="News Agent", role="Get the latest news")

agent_2 = Agent(name="Weather Agent", role="Get the weather for the next 7 days")

team = Team(name="News and Weather Team", mode="coordinate", members=[agent_1, agent_2])

# Synchronous execution
result = team.run("What is the weather in Tokyo?")

# Asynchronous execution
result = await team.arun("What is the weather in Tokyo?")
```

## Cancelling a Run

You can cancel a run by using the `cancel_run` function on the Team.

Below is a basic example that starts an team run in a thread and cancels it from another thread, simulating how it can be done via an API. This is supported via [AgentOS](/docs/agent_os) as well.

```python
import threading
import time

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.agent import RunEvent
from agno.run.base import RunStatus
from agno.run.team import TeamRunEvent
from agno.team import Team


def long_running_task(team: Team, run_id_container: dict):
    """
    Simulate a long-running team task that can be cancelled.
    """
    try:
        # Start the team run - this simulates a long task
        final_response = None
        content_pieces = []

        for chunk in team.run(
            "Write a very long story about a dragon who learns to code. "
            "Make it at least 2000 words with detailed descriptions and dialogue. "
            "Take your time and be very thorough.",
            stream=True,
        ):
            if "run_id" not in run_id_container and chunk.run_id:
                print(f"üöÄ Team run started: {chunk.run_id}")
                run_id_container["run_id"] = chunk.run_id

            if chunk.event in [TeamRunEvent.run_content, RunEvent.run_content]:
                print(chunk.content, end="", flush=True)
                content_pieces.append(chunk.content)
            elif chunk.event == RunEvent.run_cancelled:
                print(f"\nüö´ Member run was cancelled: {chunk.run_id}")
                run_id_container["result"] = {
                    "status": "cancelled",
                    "run_id": chunk.run_id,
                    "cancelled": True,
                    "content": "".join(content_pieces)[:200] + "..."
                    if content_pieces
                    else "No content before cancellation",
                }
                return
            elif chunk.event == TeamRunEvent.run_cancelled:
                print(f"\nüö´ Team run was cancelled: {chunk.run_id}")
                run_id_container["result"] = {
                    "status": "cancelled",
                    "run_id": chunk.run_id,
                    "cancelled": True,
                    "content": "".join(content_pieces)[:200] + "..."
                    if content_pieces
                    else "No content before cancellation",
                }
                return
            elif hasattr(chunk, "status") and chunk.status == RunStatus.completed:
                final_response = chunk

        # If we get here, the run completed successfully
        if final_response:
            run_id_container["result"] = {
                "status": final_response.status.value
                if final_response.status
                else "completed",
                "run_id": final_response.run_id,
                "cancelled": final_response.status == RunStatus.cancelled,
                "content": ("".join(content_pieces)[:200] + "...")
                if content_pieces
                else "No content",
            }
        else:
            run_id_container["result"] = {
                "status": "unknown",
                "run_id": run_id_container.get("run_id"),
                "cancelled": False,
                "content": ("".join(content_pieces)[:200] + "...")
                if content_pieces
                else "No content",
            }

    except Exception as e:
        print(f"\n‚ùå Exception in run: {str(e)}")
        run_id_container["result"] = {
            "status": "error",
            "error": str(e),
            "run_id": run_id_container.get("run_id"),
            "cancelled": True,
            "content": "Error occurred",
        }


def cancel_after_delay(team: Team, run_id_container: dict, delay_seconds: int = 3):
    """
    Cancel the team run after a specified delay.
    """
    print(f"‚è∞ Will cancel team run in {delay_seconds} seconds...")
    time.sleep(delay_seconds)

    run_id = run_id_container.get("run_id")
    if run_id:
        print(f"üö´ Cancelling team run: {run_id}")
        success = team.cancel_run(run_id)
        if success:
            print(f"‚úÖ Team run {run_id} marked for cancellation")
        else:
            print(
                f"‚ùå Failed to cancel team run {run_id} (may not exist or already completed)"
            )
    else:
        print("‚ö†Ô∏è  No run_id found to cancel")


def main():
    """Main function demonstrating team run cancellation."""

    # Create team members
    storyteller_agent = Agent(
        name="StorytellerAgent",
        model=OpenAIChat(id="o3-mini"),
        description="An agent that writes creative stories",
    )

    editor_agent = Agent(
        name="EditorAgent",
        model=OpenAIChat(id="o3-mini"),
        description="An agent that reviews and improves stories",
    )

    # Initialize the team with agents
    team = Team(
        name="Storytelling Team",
        members=[storyteller_agent, editor_agent],
        model=OpenAIChat(id="o3-mini"),  # Team leader model
        description="A team that collaborates to write detailed stories",
        mode="coordinate",  # Team members collaborate on the task
    )

    print("üöÄ Starting team run cancellation example...")
    print("=" * 50)

    # Container to share run_id between threads
    run_id_container = {}

    # Start the team run in a separate thread
    team_thread = threading.Thread(
        target=lambda: long_running_task(team, run_id_container), name="TeamRunThread"
    )

    # Start the cancellation thread
    cancel_thread = threading.Thread(
        target=cancel_after_delay,
        args=(team, run_id_container, 8),  # Cancel after 8 seconds
        name="CancelThread",
    )

    # Start both threads
    print("üèÉ Starting team run thread...")
    team_thread.start()

    print("üèÉ Starting cancellation thread...")
    cancel_thread.start()

    # Wait for both threads to complete
    print("‚åõ Waiting for threads to complete...")
    team_thread.join()
    cancel_thread.join()

    # Print the results
    print("\n" + "=" * 50)
    print("üìä RESULTS:")
    print("=" * 50)

    result = run_id_container.get("result")
    if result:
        print(f"Status: {result['status']}")
        print(f"Run ID: {result['run_id']}")
        print(f"Was Cancelled: {result['cancelled']}")

        if result.get("error"):
            print(f"Error: {result['error']}")
        else:
            print(f"Content Preview: {result['content']}")

        if result["cancelled"]:
            print("\n‚úÖ SUCCESS: Team run was successfully cancelled!")
        else:
            print("\n‚ö†Ô∏è  WARNING: Team run completed before cancellation")
    else:
        print("‚ùå No result obtained - check if cancellation happened during streaming")

    print("\nüèÅ Team cancellation example completed!")


if __name__ == "__main__":
    # Run the main example
    main()
```

For a more complete example, see [Cancel a run](https://github.com/agno-agi/agno/blob/main/cookbook/teams/basic/team_cancel_a_run.py).


## RunOutput

The `Team.run()` function returns a `TeamRunOutput` object when not streaming. Here are some of the core attributes:
- `run_id`: The id of the run.
- `team_id`: The id of the team.
- `team_name`: The name of the team.
- `session_id`: The id of the session.
- `user_id`: The id of the user.
- `content`: The response content.
- `content_type`: The type of content. In the case of structured output, this will be the class name of the pydantic model.
- `reasoning_content`: The reasoning content.
- `messages`: The list of messages sent to the model.
- `metrics`: The metrics of the run.  For more details see [Metrics](/docs/documentation/teams/metrics).
- `model`: The model used for the run.
- `member_responses`: The list of member responses. Optional to add when `store_member_responses=True` on the `Team`.

See detailed documentation in the [TeamRunOutput](/reference/teams/team-run-output) documentation.

### Using an Output Model

You can use a different model to produce the run output of the team.
This is useful when the primary model is optimized for image analysis, for example, but you want a different model to produce a structured output response.

```python
team = Team(
    model=Gemini(id="gemini-2.0-flash-001"),  # The main processing model
    description="You write movie scripts.",
    output_schema=MovieScript,
    output_model=OpenAIChat(id="o3-mini"),  # Only used to parse the output
    members=[...],
)
```

You can also provide a custom `output_model_prompt` to your Output Model to customize the model's instructions.

<Tip>
Gemini models often reject requests to use tools and produce structured output at the same time. Using an Output Model is an effective workaround for this.
</Tip>

## Streaming Responses

To enable streaming, set `stream=True` when calling `run()`. This will return an iterator of `TeamRunOutputEvent` objects instead of a single response.

```python
from agno.team import Team
from agno.models.openai import OpenAIChat

agent_1 = Agent(name="News Agent", role="Get the latest news")

agent_2 = Agent(name="Weather Agent", role="Get the weather for the next 7 days")

team = Team(name="News and Weather Team", mode="coordinate", members=[agent_1, agent_2])

# Synchronous execution
for chunk in team.run("What is the weather in Tokyo?", stream=True, stream_intermediate_steps=True):
    print(chunk.content, end="", flush=True)

# Asynchronous execution
async for chunk in team.arun("What is the weather in Tokyo?", stream=True, stream_intermediate_steps=True):
    print(chunk.content, end="", flush=True)
```

### Streaming Intermediate Steps

Throughout the execution of a team, multiple events take place, and we provide these events in real-time for enhanced team transparency.

You can enable streaming of intermediate steps by setting `stream_intermediate_steps=True`.

```python
# Stream with intermediate steps
response_stream = team.run(
    "What is the weather in Tokyo?",
    stream=True,
    stream_intermediate_steps=True
)
```

### Handling Events

You can process events as they arrive by iterating over the response stream:

```python
response_stream = team.run("Your prompt", stream=True, stream_intermediate_steps=True)

for event in response_stream:
    if event.event == "TeamRunContent":
        print(f"Content: {event.content}")
    elif event.event == "TeamToolCallStarted":
        print(f"Tool call started: {event.tool}")
    elif event.event == "ToolCallStarted":
        print(f"Member tool call started: {event.tool}")
    elif event.event == "ToolCallCompleted":
        print(f"Member tool call completed: {event.tool}")
    elif event.event == "TeamReasoningStep":
        print(f"Reasoning step: {event.content}")
    ...
```

<Note>
Team member events are yielded during team execution when a team member is being executed.  You can disable this by setting `stream_member_events=False`.
</Note>


### Storing Events

You can store all the events that happened during a run on the `RunResponse` object.

```python
from agno.team import Team
from agno.models.openai import OpenAIChat
from agno.utils.pprint import pprint_run_response

team = Team(model=OpenAIChat(id="o3-mini"), members=[], store_events=True)

response = team.run("Tell me a 5 second short story about a lion", stream=True, stream_intermediate_steps=True)
pprint_run_response(response)

for event in response.events:
    print(event.event)
```

By default the `TeamRunContentEvent` and `RunContentEvent` events are not stored. You can modify which events are skipped by setting the `events_to_skip` parameter.

For example:

```python
team = Team(model=OpenAIChat(id="o3-mini"), members=[], store_events=True, events_to_skip=[TeamRunEvent.run_started.value])
```

### Event Types

The following events are sent by the `Team.run()` and `Team.arun()` functions depending on team's configuration:

#### Core Events
| Event Type | Description |
|------------|-------------|
| `TeamRunStarted` | Indicates the start of a run |
| `TeamRunContent` | Contains the model's response text as individual chunks |
| `TeamRunCompleted` | Signals successful completion of the run |
| `TeamRunError` | Indicates an error occurred during the run |
| `TeamRunCancelled` | Signals that the run was cancelled |

#### Tool Events
| Event Type | Description |
|------------|-------------|
| `TeamToolCallStarted` | Indicates the start of a tool call |
| `TeamToolCallCompleted` | Signals completion of a tool call, including tool call results |

#### Reasoning Events
| Event Type | Description |
|------------|-------------|
| `TeamReasoningStarted` | Indicates the start of the team's reasoning process |
| `TeamReasoningStep` | Contains a single step in the reasoning process |
| `TeamReasoningCompleted` | Signals completion of the reasoning process |

#### Memory Events
| Event Type | Description |
|------------|-------------|
| `TeamMemoryUpdateStarted` | Indicates that the team is updating its memory |
| `TeamMemoryUpdateCompleted` | Signals completion of a memory update |

#### Parser Model events
| Event Type | Description |
|------------|-------------|
| `TeamParserModelResponseStarted` | Indicates the start of the parser model response |
| `TeamParserModelResponseCompleted` | Signals completion of the parser model response |

#### Output Model events
| Event Type | Description |
|------------|-------------|
| `TeamOutputModelResponseStarted` | Indicates the start of the output model response |
| `TeamOutputModelResponseCompleted` | Signals completion of the output model response |

See detailed documentation in the [TeamRunOutput](/reference/teams/team-run-output) documentation.

## Developer Resources

- View the [Team schema](/reference/teams/team)
- View the [TeamRunOutput schema](/reference/teams/team-run-output)
- View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/teams/README.md)