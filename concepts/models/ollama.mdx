---
title: Ollama
description: Learn how to use Ollama with Agno.
---

Run large language models with Ollama, either locally or through Ollama Cloud.

[Ollama](https://ollama.com) is a fantastic tool for running models both locally and in the cloud.

**Local Usage**: Run models on your own hardware using the Ollama client.

**Cloud Usage**: Access cloud-hosted models via [Ollama Cloud](https://ollama.com) with an API key. 

Ollama supports multiple open-source models. See the library [here](https://ollama.com/library).

Experiment with different models to find the best fit for your use case. Here are some general recommendations:

- `llama3.3` models are good for most basic use-cases.
- `qwen` models perform specifically well with tool use.
- `deepseek-r1` models have strong reasoning capabilities.
- `phi4` models are powerful, while being really small in size.

## Authentication (Ollama Cloud Only)

To use Ollama Cloud, set your `OLLAMA_API_KEY` environment variable. You can get an API key from [Ollama Cloud](https://ollama.com).

<CodeGroup>

```bash Mac
export OLLAMA_API_KEY=***
```

```bash Windows
setx OLLAMA_API_KEY ***
```

</CodeGroup>

When using Ollama Cloud, the host is automatically set to `https://ollama.com`. For local usage, no API key is required.

## Set up a model

### Local Usage

Install [ollama](https://ollama.com) and run a model:

```bash run model
ollama run llama3.1
```

This starts an interactive session with the model.

To download the model for use in an Agno agent:

```bash pull model
ollama pull llama3.1
```

### Cloud Usage

For Ollama Cloud, no local Ollama server installation is required. Install the Ollama library, set up your API key as described in the Authentication section above, and access cloud-hosted models directly.

## Examples

### Local Usage

Once the model is available locally, use the `Ollama` model class to access it:

<CodeGroup>

```python agent.py
from agno.agent import Agent
from agno.models.ollama import Ollama

agent = Agent(
    model=Ollama(id="llama3.1"),
    markdown=True
)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story.")
```

</CodeGroup>

### Cloud Usage

<Note>When using Ollama Cloud with an API key, the host is automatically set to `https://ollama.com`. You can omit the `host` parameter.</Note>

<CodeGroup>

```python agent.py
from agno.agent import Agent
from agno.models.ollama import Ollama

agent = Agent(
    model=Ollama(id="deepseek-v3.1:671b"),
    markdown=True
)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story.")
```

</CodeGroup>

<Note> View more examples [here](/examples/models/ollama/basic). </Note>

## Params

<Snippet file="model-ollama-params.mdx" />

`Ollama` is a subclass of the [Model](/reference/models/model) class and has access to the same params.
