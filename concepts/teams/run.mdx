---
title: Running your Team
description: Learn how to run a team and get the response.
---

The `Team.run()` function runs the team and generates a response, either as a `TeamRunOutput` object or a stream of `TeamRunOutputEvent` objects.

<Note>
  Many of our examples use `team.print_response()` which is a helper utility to
  print the response in the terminal. It uses `team.run()` under the hood.
</Note>

## Running your Team

Here's how to run your team. The response is captured in the `response` and `response_stream` variables.

```python
from agno.team import Team
from agno.models.openai import OpenAIChat

agent_1 = Agent(name="News Agent", role="Get the latest news")

agent_2 = Agent(name="Weather Agent", role="Get the weather for the next 7 days")

team = Team(name="News and Weather Team", mode="coordinate", members=[agent_1, agent_2])

# Synchronous execution
result = team.run("What is the weather in Tokyo?")

# Asynchronous execution
result = await team.arun("What is the weather in Tokyo?")
```

### RunOutput

The `Team.run()` function returns a `TeamRunOutput` object when not streaming. Here are some of the core attributes:

- `run_id`: The id of the run.
- `team_id`: The id of the team.
- `team_name`: The name of the team.
- `session_id`: The id of the session.
- `user_id`: The id of the user.
- `content`: The response content.
- `content_type`: The type of content. In the case of structured output, this will be the class name of the pydantic model.
- `reasoning_content`: The reasoning content.
- `messages`: The list of messages sent to the model.
- `metrics`: The metrics of the run. For more details see [Metrics](/concepts/teams/metrics).
- `model`: The model used for the run.
- `member_responses`: The list of member responses. Optional to add when `store_member_responses=True` on the `Team`.

See detailed documentation in the [TeamRunOutput](/reference/teams/team-response) documentation.

### Using an Output Model

You can use a different model to produce the run output of the team.
This is useful when the primary model is optimized for image analysis, for example, but you want a different model to produce a structured output response.

```python
team = Team(
    model=Gemini(id="gemini-2.0-flash-001"),  # The main processing model
    description="You write movie scripts.",
    output_schema=MovieScript,
    output_model=OpenAIChat(id="gpt-5-mini"),  # Only used to parse the output
    members=[...],
)
```

You can also provide a custom `output_model_prompt` to your Output Model to customize the model's instructions.

<Tip>
  Gemini models often reject requests to use tools and produce structured output
  at the same time. Using an Output Model is an effective workaround for this.
</Tip>

## Streaming Responses

To enable streaming, set `stream=True` when calling `run()`. This will return an iterator of `TeamRunOutputEvent` objects instead of a single response.

```python
from agno.team import Team
from agno.models.openai import OpenAIChat

agent_1 = Agent(name="News Agent", role="Get the latest news")

agent_2 = Agent(name="Weather Agent", role="Get the weather for the next 7 days")

team = Team(name="News and Weather Team", mode="coordinate", members=[agent_1, agent_2])

# Synchronous execution
for chunk in team.run("What is the weather in Tokyo?", stream=True, stream_intermediate_steps=True):
    print(chunk.content, end="", flush=True)

# Asynchronous execution
async for chunk in team.arun("What is the weather in Tokyo?", stream=True, stream_intermediate_steps=True):
    print(chunk.content, end="", flush=True)
```

### Streaming Intermediate Steps

Throughout the execution of a team, multiple events take place, and we provide these events in real-time for enhanced team transparency.

You can enable streaming of intermediate steps by setting `stream_intermediate_steps=True`.

```python
# Stream with intermediate steps
response_stream = team.run(
    "What is the weather in Tokyo?",
    stream=True,
    stream_intermediate_steps=True
)
```

### Handling Events

You can process events as they arrive by iterating over the response stream:

```python
response_stream = team.run("Your prompt", stream=True, stream_intermediate_steps=True)

for event in response_stream:
    if event.event == "TeamRunContent":
        print(f"Content: {event.content}")
    elif event.event == "TeamToolCallStarted":
        print(f"Tool call started: {event.tool}")
    elif event.event == "ToolCallStarted":
        print(f"Member tool call started: {event.tool}")
    elif event.event == "ToolCallCompleted":
        print(f"Member tool call completed: {event.tool}")
    elif event.event == "TeamReasoningStep":
        print(f"Reasoning step: {event.content}")
    ...
```

<Note>
  Team member events are yielded during team execution when a team member is
  being executed. You can disable this by setting `stream_member_events=False`.
</Note>

### Storing Events

You can store all the events that happened during a run on the `RunResponse` object.

```python
from agno.team import Team
from agno.models.openai import OpenAIChat
from agno.utils.pprint import pprint_run_response

team = Team(model=OpenAIChat(id="gpt-5-mini"), members=[], store_events=True)

response = team.run("Tell me a 5 second short story about a lion", stream=True, stream_intermediate_steps=True)
pprint_run_response(response)

for event in response.events:
    print(event.event)
```

By default the `TeamRunContentEvent` and `RunContentEvent` events are not stored. You can modify which events are skipped by setting the `events_to_skip` parameter.

For example:

```python
team = Team(model=OpenAIChat(id="gpt-5-mini"), members=[], store_events=True, events_to_skip=[TeamRunEvent.run_started.value])
```

### Event Types

The following events are sent by the `Team.run()` and `Team.arun()` functions depending on team's configuration:

#### Core Events

| Event Type         | Description                                             |
| ------------------ | ------------------------------------------------------- |
| `TeamRunStarted`   | Indicates the start of a run                            |
| `TeamRunContent`   | Contains the model's response text as individual chunks |
| `TeamRunCompleted` | Signals successful completion of the run                |
| `TeamRunError`     | Indicates an error occurred during the run              |
| `TeamRunCancelled` | Signals that the run was cancelled                      |

#### Tool Events

| Event Type              | Description                                                    |
| ----------------------- | -------------------------------------------------------------- |
| `TeamToolCallStarted`   | Indicates the start of a tool call                             |
| `TeamToolCallCompleted` | Signals completion of a tool call, including tool call results |

#### Reasoning Events

| Event Type               | Description                                         |
| ------------------------ | --------------------------------------------------- |
| `TeamReasoningStarted`   | Indicates the start of the team's reasoning process |
| `TeamReasoningStep`      | Contains a single step in the reasoning process     |
| `TeamReasoningCompleted` | Signals completion of the reasoning process         |

#### Memory Events

| Event Type                  | Description                                    |
| --------------------------- | ---------------------------------------------- |
| `TeamMemoryUpdateStarted`   | Indicates that the team is updating its memory |
| `TeamMemoryUpdateCompleted` | Signals completion of a memory update          |

#### Parser Model events

| Event Type                         | Description                                      |
| ---------------------------------- | ------------------------------------------------ |
| `TeamParserModelResponseStarted`   | Indicates the start of the parser model response |
| `TeamParserModelResponseCompleted` | Signals completion of the parser model response  |

#### Output Model events

| Event Type                         | Description                                      |
| ---------------------------------- | ------------------------------------------------ |
| `TeamOutputModelResponseStarted`   | Indicates the start of the output model response |
| `TeamOutputModelResponseCompleted` | Signals completion of the output model response  |

See detailed documentation in the [TeamRunOutput](/reference/teams/team-response) documentation.

## Developer Resources

- View the [Team schema](/reference/teams/team)
- View the [TeamRunOutput schema](/reference/teams/team-response)
- View [Cookbook](https://github.com/agno-agi/agno/tree/v2.0/cookbook/teams/README.md)
