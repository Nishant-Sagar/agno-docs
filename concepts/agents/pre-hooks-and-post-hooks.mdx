---
title: Pre-hooks and Post-hooks
description: Learn about using pre-hooks and post-hooks with your agents.
---

Pre-hooks and post-hooks are a simple way to validate or modify the input and output of an Agent run.


## Pre-hooks

You can use pre-hooks to check, validate or modify the **input** that is passed to an Agent.

Pre-hooks are executed at the beginning of a run, before the input is presented to the LLM.

In the following example, we create a pre-hook that transforms the input to be more relevant to the agent's purpose:

```python
from typing import Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.agent import RunInput
from agno.session.agent import AgentSession
from agno.utils.log import log_debug


def transform_input(
    run_input: RunInput,
    session: AgentSession,
    user_id: Optional[str] = None,
    debug_mode: Optional[bool] = None,
) -> None:
    """
    Pre-hook: Rewrite the input to be more relevant to the agent's purpose.

    This hook rewrites the input to be more relevant to the agent's purpose.
    """
    log_debug(
        f"Transforming input: {run_input.input_content} for user {user_id} and session {session.session_id}"
    )

    # Input transformation agent
    transformer_agent = Agent(
        name="Input Transformer",
        model=OpenAIChat(id="gpt-5-mini"),
        instructions=[
            "You are an input transformation specialist.",
            "Rewrite the user request to be more relevant to the agent's purpose.",
            "Use known context engineering standards to rewrite the input.",
            "Keep the input as concise as possible.",
            "The agent's purpose is to provide investment guidance and financial planning advice.",
        ],
        debug_mode=debug_mode,
    )

    transformation_result = transformer_agent.run(
        input=f"Transform this user request: '{run_input.input_content}'"
    )

    # Overwrite the input with the transformed input
    run_input.input_content = transformation_result.content
    log_debug(f"Transformed input: {run_input.input_content}")


print("üöÄ Input Transformation Pre-Hook Example")
print("=" * 60)

# Create a financial advisor agent with comprehensive hooks
agent = Agent(
    name="Financial Advisor",
    model=OpenAIChat(id="gpt-5-mini"),
    pre_hooks=[transform_input],
    description="A professional financial advisor providing investment guidance and financial planning advice.",
    instructions=[
        "You are a knowledgeable financial advisor with expertise in:",
        "‚Ä¢ Investment strategies and portfolio management",
        "‚Ä¢ Retirement planning and savings strategies",
        "‚Ä¢ Risk assessment and diversification",
        "‚Ä¢ Tax-efficient investing",
        "",
        "Provide clear, actionable advice while being mindful of individual circumstances.",
        "Always remind users to consult with a licensed financial advisor for personalized advice.",
    ],
    debug_mode=True,
)

agent.print_response(
    input="I'm 35 years old and want to start investing for retirement. moderate risk tolerance. retirement savings in IRAs/401(k)s= $100,000. total savings is $200,000. my net worth is $300,000",
    session_id="test_session",
    user_id="test_user",
    stream=True,
)
```

## Post-hooks

You can use post-hooks to check, validate or modify the **output** of an Agent run.

This is useful to get extra control over what is returned to the user, and filter out anything undesired.

In the following example, we create a post-hook that validates the response quality and safety:

```python
import asyncio

from agno.agent import Agent
from agno.exceptions import CheckTrigger, OutputCheckError
from agno.models.openai import OpenAIChat
from agno.run.agent import RunOutput
from pydantic import BaseModel


class OutputValidationResult(BaseModel):
    is_complete: bool
    is_professional: bool
    is_safe: bool
    concerns: list[str]
    confidence_score: float


def validate_response_quality(run_output: RunOutput) -> None:
    """
    Post-hook: Validate the agent's response for quality and safety.

    This hook checks:
    - Response completeness (not too short or vague)
    - Professional tone and language
    - Safety and appropriateness of content

    Raises OutputCheckError if validation fails.
    """

    # Skip validation for empty responses
    if not run_output.content or len(run_output.content.strip()) < 10:
        raise OutputCheckError(
            "Response is too short or empty",
            check_trigger=CheckTrigger.OUTPUT_NOT_ALLOWED,
        )

    # Create a validation agent
    validator_agent = Agent(
        name="Output Validator",
        model=OpenAIChat(id="gpt-4o-mini"),
        instructions=[
            "You are an output quality validator. Analyze responses for:",
            "1. COMPLETENESS: Response addresses the question thoroughly",
            "2. PROFESSIONALISM: Language is professional and appropriate",
            "3. SAFETY: Content is safe and doesn't contain harmful advice",
            "",
            "Provide a confidence score (0.0-1.0) for overall quality.",
            "List any specific concerns found.",
            "",
            "Be reasonable - don't reject good responses for minor issues.",
        ],
        output_schema=OutputValidationResult,
    )

    validation_result = validator_agent.run(
        input=f"Validate this response: '{run_output.content}'"
    )

    result = validation_result.content

    # Check validation results and raise errors for failures
    if not result.is_complete:
        raise OutputCheckError(
            f"Response is incomplete. Concerns: {', '.join(result.concerns)}",
            check_trigger=CheckTrigger.OUTPUT_NOT_ALLOWED,
        )

    if not result.is_professional:
        raise OutputCheckError(
            f"Response lacks professional tone. Concerns: {', '.join(result.concerns)}",
            check_trigger=CheckTrigger.OUTPUT_NOT_ALLOWED,
        )

    if not result.is_safe:
        raise OutputCheckError(
            f"Response contains potentially unsafe content. Concerns: {', '.join(result.concerns)}",
            check_trigger=CheckTrigger.OUTPUT_NOT_ALLOWED,
        )

    if result.confidence_score < 0.6:
        raise OutputCheckError(
            f"Response quality score too low ({result.confidence_score:.2f}). Concerns: {', '.join(result.concerns)}",
            check_trigger=CheckTrigger.OUTPUT_NOT_ALLOWED,
        )


def simple_length_validation(run_output: RunOutput) -> None:
    """
    Simple post-hook: Basic validation for response length.

    Ensures responses are neither too short nor excessively long.
    """
    content = run_output.content.strip()

    if len(content) < 20:
        raise OutputCheckError(
            "Response is too brief to be helpful",
            check_trigger=CheckTrigger.OUTPUT_NOT_ALLOWED,
        )

    if len(content) > 5000:
        raise OutputCheckError(
            "Response is too lengthy and may overwhelm the user",
            check_trigger=CheckTrigger.OUTPUT_NOT_ALLOWED,
        )


async def main():
    """Demonstrate output validation post-hooks."""
    print("üîç Output Validation Post-Hook Example")
    print("=" * 60)

    # Agent with comprehensive output validation
    agent_with_validation = Agent(
        name="Customer Support Agent",
        model=OpenAIChat(id="gpt-4o-mini"),
        post_hooks=[validate_response_quality],
        instructions=[
            "You are a helpful customer support agent.",
            "Provide clear, professional responses to customer inquiries.",
            "Be concise but thorough in your explanations.",
        ],
    )

    # Agent with simple validation only
    agent_simple = Agent(
        name="Simple Agent",
        model=OpenAIChat(id="gpt-4o-mini"),
        post_hooks=[simple_length_validation],
        instructions=[
            "You are a helpful assistant. Keep responses focused and appropriate length."
        ],
    )

    # Test 1: Good response (should pass validation)
    print("\n‚úÖ Test 1: Well-formed response")
    print("-" * 40)
    try:
        await agent_with_validation.aprint_response(
            input="How do I reset my password on my Microsoft account?"
        )
        print("‚úÖ Response passed validation")
    except OutputCheckError as e:
        print(f"‚ùå Validation failed: {e}")
        print(f"   Trigger: {e.check_trigger}")

    # Test 2: Force a short response (should fail simple validation)
    print("\n‚ùå Test 2: Too brief response")
    print("-" * 40)
    try:
        # Use a more constrained instruction to get a brief response
        brief_agent = Agent(
            name="Brief Agent",
            model=OpenAIChat(id="gpt-4o-mini"),
            post_hooks=[simple_length_validation],
            instructions=["Answer in 1-2 words only."],
        )
        await brief_agent.aprint_response(input="What is the capital of France?")
    except OutputCheckError as e:
        print(f"‚ùå Validation failed: {e}")
        print(f"   Trigger: {e.check_trigger}")

    # Test 3: Normal response with simple validation
    print("\n‚úÖ Test 3: Normal response with simple validation")
    print("-" * 40)
    try:
        await agent_simple.aprint_response(
            input="Explain what a database is in simple terms."
        )
        print("‚úÖ Response passed simple validation")
    except OutputCheckError as e:
        print(f"‚ùå Validation failed: {e}")
        print(f"   Trigger: {e.check_trigger}")


if __name__ == "__main__":
    asyncio.run(main())
```

## Guardrails

A popular use case for pre-hooks are Guardrails: built-in safeguards for your Agents.

You can learn more about them in the [Guardrails](/concepts/agents/guardrails) section.


## Developer Resources

- View [Examples](/examples/concepts/agents/pre-hooks-and-post-hooks)
- View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/agents/hooks)