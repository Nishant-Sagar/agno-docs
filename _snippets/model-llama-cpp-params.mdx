| Name | Type | Default | Description |
|------|------|---------|-------------|
| id | `str` | `"ggml-org/gpt-oss-20b-GGUF"` | The model identifier |
| name | `str` | `"LlamaCpp"` | The name of this chat model instance |
| provider | `str` | `"LlamaCpp"` | The provider of the model |
| base_url | `str` | `"http://127.0.0.1:8080/v1"` | The base URL for the LlamaCpp server |
| api_key | `str` | `None` | API key for authentication (if required) |
| max_tokens | `int` | `None` | Maximum number of tokens to generate |
| temperature | `float` | `None` | Temperature for response randomness (0.0 to 2.0) |
| top_p | `float` | `None` | Top-p sampling parameter |
| top_k | `int` | `None` | Top-k sampling parameter |
| frequency_penalty | `float` | `None` | Frequency penalty (-2.0 to 2.0) |
| presence_penalty | `float` | `None` | Presence penalty (-2.0 to 2.0) |
| stop | `List[str]` | `None` | Stop sequences |
| stream | `bool` | `False` | Enable streaming responses |
| timeout | `int` | `None` | Request timeout in seconds |
