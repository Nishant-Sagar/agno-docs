---
title: Simple Agent Evals
description: Learn how to evaluate your Agno Agents and Teams across three key dimensions - accuracy (using LLM-as-a-judge), performance (runtime and memory), and reliability (tool calls).
sidebarTitle: Overview
---

**Evals** is a way to measure the quality of your Agents and Teams. Agno provides 3 dimensions for evaluating Agents:

## Evaluation Dimensions

<CardGroup cols={3}>
  <Card title="Accuracy" icon="bullseye" href="/evals/accuracy">
    How complete/correct/accurate is the Agent's response using LLM-as-a-judge methodology.
  </Card>
  <Card title="Performance" icon="stopwatch" href="/evals/performance">
    How fast does the Agent respond and what's the memory footprint?
  </Card>
  <Card title="Reliability" icon="shield-check" href="/evals/reliability">
    Does the Agent make the expected tool calls and handle errors gracefully?
  </Card>
</CardGroup>

## Quick Start

Here's a simple example of running an accuracy evaluation:

```python quick_eval.py
from typing import Optional
from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

# Create an evaluation
evaluation = AccuracyEval(
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(model=OpenAIChat(id="gpt-4o"), tools=[CalculatorTools(enable_all=True)]),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
    additional_guidelines="Agent output should include the steps and the final answer.",
)

# Run the evaluation
result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8
```

## Best Practices

- **Start Simple:** Begin with basic accuracy tests before moving to complex performance and reliability evaluations
- **Use Multiple Test Cases:** Don't rely on a single test case - create comprehensive test suites
- **Track Over Time:** Monitor your eval results as you make changes to your agents
- **Combine Dimensions:** Use all three evaluation dimensions for a complete picture of agent quality

## Next Steps

Dive deeper into each evaluation dimension:

1. **[Accuracy Evals](/evals/accuracy)** - Learn LLM-as-a-judge techniques and multiple test case strategies
2. **[Performance Evals](/evals/performance)** - Measure latency, memory usage, and compare different configurations  
3. **[Reliability Evals](/evals/reliability)** - Test tool calls, error handling, and rate limiting behavior
